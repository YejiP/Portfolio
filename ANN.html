<!doctype html>

<style>


    #expic{

      float : bottom;
        padding-left :10%;
        padding-right:10%;
        align-items: center;
    }
    #title {
        padding-left :15%;
        padding-right :10%;
        float:bottom;}

    #n {
        padding-left :0%;
        padding-right :10%;
        float:left;
        text-align:center;
    }


    #explanation {
        padding-left :10%;
        padding-right :10%;

    }
    #cen {
      text-size-adjust: none;
      min-width: 800px;
        }
</style>
<div id = cen>
<br><br><br><br>
<div align = "center"><img src ="img/me.png" width = 100px></div><br>
<h5><center>Yeji Park</center></h5>

<center><h1>Character Recognition</h1><h4>(using Artificial Neural Network)</h4></center>
<center><a href = "ANN/demo.html"><h2>Click to run demo</h2></a>
<a href= "https://github.com/YejiP/projects/tree/master/Neural_network/src/ann">Github repository</a></center><br>
<div align = "center"><img src = "img/bar.png" width = 82% height = 10 px></div>

<center><h3>0. Problem description</h3><br></center>

<div id =explanation >According to given picture which contains one written number among 0 to 9, It will recognize the number from the picture.
<br> <b>Ex)</b> The picture below is the size of 28*28, has 784 pixels. Each pixel contains information of location and the brightness.
<br>By combining these two information, the picture is determined what number it is. Forwawrd and back propagation are used when the model is made.
</div>
<div id =explanation><img src = "ANN/ann_concept.PNG" width = 800 px></div>


<div id = title><h1>Forward propagation</h1></div>
<div id =expic><img src = "ANN/forward.PNG" width = 500 px align = "left"><br><br><br>Forward propagation is inferring the value using a model created based on input values and answer.<br>
Each node in the neral network is called perceptron.
Input perceptrons affect perceptrons in the next layer, and this perceptrons in the next layer affects perceptrons in its next layer.<br><br><br><br><br></div>
<br><br>
<div id =expic><img src = "ANN/for_mo.PNG" width = 400 px align = "left">Each perceptrons in the input layer are used to make perceptron in the next layer. The values of each perceptron are multiplied by weights and is summed up to make perceptrons in the next layer. <br>
<br>The result of summation then becomes the input of activation function such as sigmoid, ReLu, and softmax(only at the last layer, in classification problems).<br>
Creating the right model is largely related to finding right weights. Input values are given, therefore by changing weights, proper model can be made.<br><br>
Evaluating errors is different based on the type of problems. Usually, Regression uses sigmoid function to evaluate erros, and Classification uses reLu and sigmoid function.
Since we are dealing with the classification problem, we will look how to back propagate ANN that uses ReLu and Softmax function.
<br>

Then, How can we find proper weights? It can be done by evaluating errors of a current model, and changing its weights based on errors.
Back propagation can conduct this action.
However, We cannot find proper weights at one go. The initial weights are randomized and then it slowly changes over time. </div>
<br><br>


<div id = explanation>
  In the classification problems, the last output is driven by using cross-entropy and softmax function.
  <br><br>
  <img src = "ANN/soft_max.PNG" width = 600 align =  left>
  Let's assume that we have 4 classes(A,B,C,D), and each outcome is 9, 5, 3, 5 respectively (The outcome is something like a1*w1 + a2*w2 + bias)<br><br>
  We can simply put this as a probability,
  (Roughly rounded)<br>
  A : 9/(9+5+3+5) = 0.4<br>
  B : 5/(9+5+3+5) = 0.23<br>
  C : 3/(9+5+3+5) = 0.14<br>
  D : 5(9+5+3+5) = 0.23<br><br>

  However, Instead of this, we use Euler's number to present the probability for each class.<br>
  (The reason for using Euler's number is closely related to the back propagation(makes easier to differentiate), and the cross entropy error function.)<br>
<br><br>
  Now, (e<sup>9</sup> + e<sup>5</sup> + e<sup>3</sup> + e<sup>5</sup>)  is the denominator for each class, and the numerator is e<sup>9</sup> , e<sup>5</sup> , e<sup>3</sup> , e<sup>5</sup> for each class.<br>
  A : e<sup>9</sup>/(e<sup>9</sup> + e<sup>5</sup> + e<sup>3</sup> + e<sup>5</sup>) = 0.96<br>
  B : e<sup>5</sup>/(e<sup>9</sup> + e<sup>5</sup> + e<sup>3</sup> + e<sup>5</sup>)= 0.017<br>
  C : e<sup>3</sup>/(e<sup>9</sup> + e<sup>5</sup> + e<sup>3</sup> + e<sup>5</sup>) = 0.0023<br>
  D : e<sup>5</sup>/(e<sup>9</sup> + e<sup>5</sup> + e<sup>3</sup> + e<sup>5</sup>) = 0.017<br><br>
  y= e<sup>x</sup> is a monotone increasing function, so It does not change the magnitude of the relation.
  (At the end of the day, our purpose is just to pick up the class with the highest probability)
<br>
  However, the problem lies when the number gets too big.
  we need to modify this number.
  A:9  B:5  C:3  D:5 .<br>
  First, find the biggiest number which is 9(A) and then subtract 9 for all classes.
  <br>A: 0  B:-4  C:-6  D:-4<br>

<br>
  A : e<sup>0</sup>/(e<sup>0</sup> + e<sup>-4</sup> + e<sup>-6</sup> + e<sup>-4</sup>) = 0.96<br>
  B : e<sup>-4</sup>/(e<sup>0</sup> + e<sup>-4</sup> + e<sup>-6</sup> + e<sup>-4</sup>) = 0.017<br>
  C : e<sup>-6</sup>/(e<sup>0</sup> + e<sup>-4</sup> + e<sup>-6</sup> + e<sup>-4</sup>)  = 0.0023<br>
  D : e<sup>-4</sup>/(e<sup>0</sup> + e<sup>-4</sup> + e<sup>-6</sup> + e<sup>-4</sup>) = 0.017<br>
  (The probability is still the same.)
<br>
  Conclusion
  outcome -- softmax function --> probability <br>
  We first need to know how much the error this model has. To measure this error, Crossentropy function is often used (in classification problems.)
  <br> After using softmax function, we could convert each number into possibility of the each class.
  Now, let's assume that, this picture belongs to class A. Our model was pretty close, as the final outcome was 0.96 for class A.
  However, if the picture belongs to class B, then the model is significantly wrong because the possibility of belonging in class B is only 0.017.<br>
  The error is calculated by multiplying two numbers, - ln(The possility of beloning in each class) * Correct answer, and summing up.
   if the estimation is close enough , then it will get lower costs. if it is not close, then the loss will be high.</div>
<div id = "expic"><img src = "ANN/crossentropy.PNG" width = 550 ><img src = "ANN/-lnx.PNG" width=200px ><br> If x is close to 0, then the cost will be close to infinity. if it is close to 1, then the cost will be 0.
<br>
   <br>
   let's assume correct answer is (A,B,C,D)= (1,0,0,0)
   Then,
   cost for A = 1*(-ln(0.96+0.002)) = 1*(0.0406)= 0.0406 . so the cost is very low.
   cost for B,C,D = 0*(-ln(...))= 0  (So, In cross entropy error, we only care about how close the correct answer is with estimation, don't care about how correct(wrong) is for BCD.))

   If, the probability of A was not 0.96 but 0, then ?
   Cost for A = 1*(-ln(0+0.002)) = 1*(6.2146)= 6.2146 . so the cost is very high compared to previous one.
   also, this is reason why we should add small numbers like 0.002, because -ln0 is infinite.
   <img src = "ANN/tot_err.PNG" width=600px>
</div>

<div id = "explanation">
  <a onclick="this.nextSibling.style.display=(this.nextSibling.style.display=='none')?'block':'none';" href="javascript:void(0)">
  (Click more to read) When the last output was from the sigmoid function
  </a><div style="DISPLAY: none">
    <br><b>How do we know how much errors we have in this model?</b><br>
    <u>If we simply add the errors? </u>
    <br>0.2+0.1+0.2+(-0.9)+(0.05)+0.25+0+0.05+0.05+0 =  0.5 +(-0.9) +0.3 = <b>-0.01</b><br>
    the errors is only -0.01. It is becuase some nodes have different sign, so they cancel out each other's errors.<br>
    <br>To avoid this, Squaring each errors and then add.
    <br>
    <img src = "ANN/sig_err.PNG" width = 800>
    0.2<sup>2</sup>+0.1<sup>2</sup>+0.2<sup>2</sup>+(-0.9)<sup>2</sup>+(0.05)<sup>2</sup>+0.25<sup>2</sup>+0<sup>2</sup>+0.05<sup>2</sup>+0.05<sup>2</sup>+0 <sup>2</sup>= <b>1.01</b>
    <br>The evaluation function we will use here is 1/2(correct answer-estimated answer)<sup>2</sup>
    (1/2)*1.01 = 0.505 <b>error for this case</b>
  </div>
    <br>

    <br><br>
</div>

<div id = title><h1>Back-propagation</h1></div>
<div id = expic><img src = "ANN/back.PNG" width = 400 px align = "left">Back-propagation is the process of neural network finding right weights.Depending on how much each node contributes to the total error.
    we will focus on <b>1. How much error each node contributes  2. How much weight should be updated</b>
    <br>Back-propagation starts from the nodes from the output layer. These nodes can be evaluated by seeing how different it is from the right answer<br>    <img src = "ANN/e2.PNG" width = 800px><br>
  Even we do not explicitly see w11 from the equation above, cost function contains w11, as the A contains w11.<br> <img src = "ANN/e1.PNG" height=40px><br> Therefore, we can differentiate the cost function with respective to w11(Weights).<br>
The equation below is showing the differentiation of the cost function. It also means the moment slope, and derivitives.
  <img src = "ANN/e0.PNG" height = 50px ><br>
  purpose is to minimize 'y' value, since y stands for the costs(total errors of all nodes)<br>
If the value of moment slope is positive, then when w11 goes to positive side, cost increases. If the value of moment slope is negative, then when w11 goes to positive side, cost decreases.
The former case, w11 should go to negative side and the later case, w11 should go to positive side to decrease overall cost.<br>
<br>
Now, Let's elaborate on how to update weights. let's assume specific situation.
There are 4 classes as output, and let's name it class 1, 2,3 and 4 from the top to the bottom.<br>
<img src = "ANN/tot_err.PNG" width=600px>
  <br>The picture belongs to class 1, so the correct answer for each class is 1,0,0,0 respectivley.
  we can simplify it as the eqation below, since only one is multplied by 1 , and others by 0.<br>
  <img src = "ANN/e2.PNG" width=800px><br>
  <img src = "ANN/tc.PNG" height=50px>
<br> First, we will update w11. To update w11, we first need to calculate the derivities. It can be done by following the chain rule.<br>
  <img src = "ANN/wu11.PNG"  width=600px><br>

<img src = "ANN/w11u.PNG" height=300px><br>
-(1- A's probability)*a1 is the result of d(cost)/d(w11). If the derivitive(moment slope) is positive value, then we need to go to the opposite the direction of the value of derivities to reduce the cost, and if it is negative value, also we need to go to the opposite direction of the value of deritivies.
Either way, it needs to head the opposite direction of the value of derivities.
so,w11 +(1- A's probability)*a1. It is not compelete formula, because we need to multiply appropriate small number called alpha(0.001 ,0.1...) to prevent to miss or skip the point we want to find.
 <b>w11 -alpha*((A's probability-1)*a1).</b>

    <br><br><br>
    Now, let's update w21.It is similar but different since the cost function contains w21 only at the denominator. (It is different wiht updating w11, since the cost function contains w11 at the numerator and also at the denominator).
    So differentiation result is different.<br>
    <img src = "ANN/wu21.PNG"  width=600px><br>
    <img src = "ANN/w21u.PNG" height=300px><br>
    (B's probability-0)*a2 is the result of d(cost)/d(w21). So, the weight update would be <b>w21 +alpha*(B's probability*a2).</b>
<br>
    By these two cases, we can generalize the way of updating weights.
    <h2>weight - alpha*(probability- correct answer)*the previous output</h2>
    <br>we can think (probability- correct answer) as a error of the output node. Now, to change the weights of the parevious layer, we need to know the error of the previous layer's node.
    Caculation is simple, the right picture below .
    <br><img src = "ANN/out_err.PNG" width = 400px><img src = "ANN/a1.PNG" width = 300px>
    <br>This is a basic pattern of machine learning. There are some more contents such as how to initialize good weights,(Xavier, He), regularization, and dropout.
    this contents will be updated in the future. <br>
</div>
</div>
