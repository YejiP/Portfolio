<!doctype html>


<style>
    #text {
            float : bottom;
            padding-left :10%;
            padding-right : 10%;
            text-align:left;
            }

      #expic{

            float : bottom;
            padding-left :10%;
            padding-right:10%;
            align-items: center;
                }

    #pic{
        padding-top: 2%;
        padding-right:10%;
        text-align:center;
        float :bottom;
        }

    #title {
            padding-left :10%;
            padding-right :10%;
            float:bottom;
          }
          #cen {
            min-width: 800px;
            text-size-adjust: none;

              }
</style>
<div id = cen>
<br><br><br><br>
<div align = "center"><img src ="img/me.png" width = 100px></div><br>
<h5><center>Yeji Park</center></h5>

<center><h1>Recommendation system</h1><h4>(Clustering Algorithm)</h4></center>
<center><div id = title><a href = "recommend/result.html"><h2>Click to watch results</h2></a></div>
<a href = "">Github repository</a></center><br>
<div align = "center"><img src = "img/bar.png" width = 82% height = 10 px></div>

<div id =title ><div align = "center"><h3>0. Problem description</h3><br></div>
1. K-Means

Unsupervised learning
Clustering

-  Label 이 없는 데이터를 학습한다.
-  feature 들을 가지고, 숨은 구조를 찾기 위해 사용한다.
-  unsupervised learning 자체가 하나의 목적이 될 수 있다. (뉴스 비슷한 주제로 묶어주거나 쇼핑)
-  다음 단계를 위한 전처리 (supervised learning 을 위해 label 만들어 줄 수 있다.)
-  clustering 자체가 label이 될 수 있다.

K-means clustering


- k개의 군집으로 나누려고 한다. K는 우리가 임의로 정해준다.
- centroid 는, 군집의 중심이 되는 값이다. k개가 있다.
- centroid 를 먼저 잡을 때는, 여러가지 방법이 있다. Centroid를 모두 같은 값으로 초기화 시키는 것은 좋은 방법이 아니다.
- 자료 값에서 랜덤하게 선택하는 방법도 있다.
- normalization 해주어야 한다. (유클리드 거리 쓰니까.. 한 변수가 너무 커버릴 때, 다른 변수의 영향력이 축소되어 나타난다.)
-  z normalization :
(x-mean(x))/sigma

-  min max normalization:
(x- xmin)/ (x max – x min)



과정

1. Random 한 값으로 centroid 를 잡아준다.
2. 각각의 자료 값이 어떤 centroid 에 가까운지 그룹 지어준다.
3. 그룹끼리 mean 좌표를 다시 잡아준다.
4. 이 mean 값들과 자료 값들과의 거리를 다시 측정해서, 다시 그룹지어준다.
5. mean 값들이 변하지 않을 때까지 반복해준다.






Calinski-Harabasz index


WSS(k)?
= 각각의 cluster에 속한 자료가 그 cluster의 중심과 얼마나 가까운지 보여주는 지표이다.
예를들어 클러스터가, 5개 있다고 하자.
첫번째 클러스터를 생각해보자. 이 첫번째 클러스터에 속하는 값들이 있을 것이다. 이것들의 mean 을 구해서, 이 mean 과의 거리가 얼마나 떨어져있는지 유클리디안 거리를 이용해서 구한다. 그리고 이것을 더해준다.
이렇게, 모든 클러스터에 대해 구해주고 그 값을 더해준다.


BSS(K)?
= TSS(K) – WSS(K)
Cluster 간 거리를 의미한다.


TSS(k)?
Cluster 의 개수와 상관없이 항상 값이 같다.
모든 자료의 평균 값과 모든 자료가 얼마나 떨어져있는지를 본다.


CH index
크다는 의미는, bss 값이 크고, wss 가 작다는 것이니까, clustering 이 잘되었다는 것을 의미한다.


2. KNN


3. REecommendation system using KNN
 Content based filtering
-컨텐츠에 기반해서, 사용자가 어떤 것을 좋아하는지 판단하고 추천해주는 시스템이다.

한계
- 컨텐츠마다, 어떤것이 의미있는지 정해줘야한다.
- 퀄리티 측면에서는 떨어질 수 있다.
-새롭고 다양한 아이템을 잘 추천하지 않는다.

Collaborative filtering
-memory based CF/model based  CF 가 있다.
: 특정한 structure 가지고 있지만, 다 똑같지는 않다.

MEMORY BASED CF:
아이템에 대해, 사용자의 RATING 을 추측할 때, 비슷한 유저나 아이템의 정보를 이용하는 것이다.

MODEL BASED CF:
아이템에 대해, 사용자의 RATING 을 추측할 때, 모델을 만들어 그 모델에 의해 추측하는 것이다. (CLUSTERING 같은)

- 신상은 추천 못해준다.
 cold start problem ( 시작할 때 문제가 생긴다.)
--> 노출을 시킨다.
--> 평가단 활용

- 계산이 비싸다.
knn 이니까, centroid 찾을 때도...
한번 추천해주는데 계산이 ..
미리 계산해놓기 어렵. 실시간으로 값이 많이 바뀌기 때문에.
computationally expensive = not scalable( 데이터가 커져도 할 수 있는 알고리즘이 아니다)
데이터 커지면, 느려서 못쓴다.

유사도 측정

Item based CF

K =n
아이템끼리의 유사성을 본다. 가장 유사한 아이템 n개를 정한다.
그리고, 한 아이템에 대해서, 사용자가 구매한 아이템이 몇 개인지보고, 그거에 대한 상관성을 계산해준다.

</div>




</div>
